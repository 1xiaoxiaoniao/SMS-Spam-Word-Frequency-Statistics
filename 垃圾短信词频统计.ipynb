{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'买', 20000), (u'现成', 10000), (u'总价', 10000), (u'独立', 10000), (u'租金', 10000), (u'涨', 10000), (u'6575162251116', 10000), (u'港', 10000), (u'收', 10000), (u'物流', 10000), (u'市政', 10000), (u'万多', 10000), (u'余万', 10000), (u'满租旺', 10000), (u'4000287878', 10000), (u'赚', 10000), (u'询', 10000), (u'商家', 10000), (u'全', 10000), (u'国际', 10000), (u'定点', 10000), (u'大华', 10000), (u'铺', 10000), (u'双证齐', 10000)]\n"
     ]
    }
   ],
   "source": [
    "import jieba \n",
    "import codecs\n",
    "import re \n",
    "from collections import Counter\n",
    "\n",
    "inputwords_file = \"C:/Users/51457/Documents/spam.txt\"   \n",
    "stopwords_file = \"C:/Users/51457/Documents/stopwords.dat\"\n",
    "\n",
    "## 读取文件\n",
    "def readfile(filepath):\n",
    "    fp = codecs.open(filepath, \"r\", encoding=\"utf-8\")\n",
    "    content = fp.read()\n",
    "    fp.close()\n",
    "    return content\n",
    "\n",
    "## 保存文件\n",
    "def savefile(savepath, content):\n",
    "    fp = codecs.open(savepath,\"w\",encoding='utf-8')\n",
    "    fp.write(content)\n",
    "    fp.close()\n",
    "    \n",
    "## 创建停用词表\n",
    "def stopwordslist(filepath):\n",
    "    stopwords = readfile(filepath).splitlines()\n",
    "    return stopwords\n",
    "\n",
    "## 加载输入文本,并对其进行分词\n",
    "def inputwordslist(filepath):\n",
    "    inputwords = readfile(filepath).splitlines()\n",
    "    return inputwords\n",
    "\n",
    "## 去除输入文本中的网址数据 \n",
    "## 顺便把换行符和空格符也去了\n",
    "def filter_url_tag(urlstring): \n",
    "    results = re.compile(r'http://[a-zA-Z0-9.?/&=:]*',re.S)\n",
    "    return results.sub(\"\", urlstring).replace('\\n','').replace(' ','').replace(u'\\u2605','').replace('10','')\n",
    "\n",
    "cutwordslist = []\n",
    "stopwords = stopwordslist(stopwords_file)\n",
    "\n",
    "for url_line in inputwordslist(inputwords_file):  \n",
    "    line = filter_url_tag(html_line)\n",
    "    cutwordslist += [word for word in jieba.cut(line, cut_all=False) if word not in stopwords]\n",
    "\n",
    "cutwords = dict(Counter(cutwordslist))\n",
    "\n",
    "outputwords = {}\n",
    "for k, v in cutwords.items():\n",
    "    if k in outputwords.keys():\n",
    "        outputwords[k] += v\n",
    "    else:\n",
    "        outputwords[k] = v\n",
    "\n",
    "outputwords_sorted = sorted(outputwords.items(), key= lambda x : x[1], reverse=True)[:100]\n",
    "## print outputwords_sorted\n",
    "## 使输出能正常显示中文字符\n",
    "print(repr(outputwords_sorted).decode('unicode-escape'))         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
